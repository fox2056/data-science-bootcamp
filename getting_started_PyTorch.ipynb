{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQtL75MmTKjUOvwRlx0BaC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fox2056/data-science-bootcamp/blob/main/getting_started_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "8M3hxkEHPCWT",
        "outputId": "39443e57-a68b-4280-9e89-6ada0fc92756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0+cu126'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.tensor([1,2,3])\n",
        "print(t1)\n",
        "t2 = torch.tensor([[1,2], [3,4]])\n",
        "print(t2)"
      ],
      "metadata": {
        "id": "0oIjBWXKQEJQ",
        "outputId": "f4520fa4-d29d-48a4-db4e-e034ea997733",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np_array = np.array([5,6,7])\n",
        "t3 = torch.tensor(np_array)\n",
        "print(t3)"
      ],
      "metadata": {
        "id": "JiDq-BoaQhGL",
        "outputId": "899d03be-40a5-49cd-fd00-86a7b59daa7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5, 6, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t4 = torch.zeros((3,3))\n",
        "print(t4)\n",
        "t5 = torch.ones((3,3))\n",
        "print(t5)\n",
        "t6 = torch.eye(3)\n",
        "print(t6)"
      ],
      "metadata": {
        "id": "EcKJyrleQrQA",
        "outputId": "164d616f-f609-4282-fd3c-03ae0418f48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = t1 + t3\n",
        "print(result)\n",
        "result = t3 - t1\n",
        "print(result)\n",
        "result = t1 * t3\n",
        "print(result)\n",
        "result = t3 / t1\n",
        "print(result)"
      ],
      "metadata": {
        "id": "v2UVDmqkQ2-l",
        "outputId": "264a192d-c121-4d4b-87f8-3732ec315980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 6,  8, 10])\n",
            "tensor([4, 4, 4])\n",
            "tensor([ 5, 12, 21])\n",
            "tensor([5.0000, 3.0000, 2.3333])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t7 = torch.arange(9)\n",
        "t8 = t7.reshape((3,3))\n",
        "print(t8)"
      ],
      "metadata": {
        "id": "uryvMAU7RFPT",
        "outputId": "187c9d6e-0e80-4643-e5a9-307a4df81e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = torch.dot(t1, t3)\n",
        "print(dot_product)"
      ],
      "metadata": {
        "id": "325gmIYCRRRX",
        "outputId": "207e57bd-7228-4834-9dbf-c3b038f5992f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(100, 10)\n",
        "Y = torch.randn(100, 1)"
      ],
      "metadata": {
        "id": "WwzfD343RnuZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 10\n",
        "hidden_size = 5\n",
        "output_size = 1\n",
        "\n",
        "W1 = torch.randn(hidden_size, input_size).requires_grad_()\n",
        "b1 = torch.zeros(hidden_size, requires_grad=True)\n",
        "W2 = torch.randn(output_size, hidden_size).requires_grad_()\n",
        "b2 = torch.zeros(output_size, requires_grad=True)\n",
        "\n",
        "def simple_neural_net(x, W1, b1, W2, b2):\n",
        "  z1 = torch.mm(x, W1.t()) + b1\n",
        "  a1 = torch.sigmoid(z1)\n",
        "  z2 = torch.mm(a1, W2.t()) + b2\n",
        "  return z2"
      ],
      "metadata": {
        "id": "FZ-ncICTSgL2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "epochs = 100\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  y_pred = simple_neural_net(X, W1, b1, W2, b2)\n",
        "  loss = loss_fn(y_pred.squeeze(), Y)\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    W1 -= lr * W1.grad\n",
        "    b1 -= lr * b1.grad\n",
        "    W2 -= lr * W2.grad\n",
        "    b2 -= lr * b2.grad\n",
        "\n",
        "  W1.grad.zero_()\n",
        "  b1.grad.zero_()\n",
        "  W2.grad.zero_()\n",
        "  b2.grad.zero_()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} \\t Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "e0yzd3UzTHsX",
        "outputId": "4368b967-5830-4b5d-a628-96063604b7d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \t Loss: 1.8787028789520264\n",
            "Epoch: 10 \t Loss: 1.5800961256027222\n",
            "Epoch: 20 \t Loss: 1.4522567987442017\n",
            "Epoch: 30 \t Loss: 1.3879915475845337\n",
            "Epoch: 40 \t Loss: 1.3482685089111328\n",
            "Epoch: 50 \t Loss: 1.318770170211792\n",
            "Epoch: 60 \t Loss: 1.2941666841506958\n",
            "Epoch: 70 \t Loss: 1.2724082469940186\n",
            "Epoch: 80 \t Loss: 1.2526501417160034\n",
            "Epoch: 90 \t Loss: 1.2344971895217896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([100, 1])) that is different to the input size (torch.Size([100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a feedforward neural network"
      ],
      "metadata": {
        "id": "1c3zsDSAUGSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "KSQ1a5-xTm6g"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(10, 5)\n",
        "    self.fc2 = nn.Linear(5, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "vRylwKq1UJVm",
        "outputId": "112e9bbd-1c16-4d45-8922-b24dc29e6bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "bpAgY1CBXMh_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  output = net(X)\n",
        "  loss = loss_fn(output, Y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "yjqpfT2nXypL",
        "outputId": "566c8b61-0a48-4a1d-a553-1a6b9c816d99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3465931415557861\n",
            "Epoch 2, Loss: 1.3169891834259033\n",
            "Epoch 3, Loss: 1.2897229194641113\n",
            "Epoch 4, Loss: 1.2645152807235718\n",
            "Epoch 5, Loss: 1.241209864616394\n",
            "Epoch 6, Loss: 1.2196192741394043\n",
            "Epoch 7, Loss: 1.1996023654937744\n",
            "Epoch 8, Loss: 1.1809840202331543\n",
            "Epoch 9, Loss: 1.1636667251586914\n",
            "Epoch 10, Loss: 1.1475356817245483\n",
            "Epoch 11, Loss: 1.1325218677520752\n",
            "Epoch 12, Loss: 1.1185805797576904\n",
            "Epoch 13, Loss: 1.1055517196655273\n",
            "Epoch 14, Loss: 1.0933644771575928\n",
            "Epoch 15, Loss: 1.081954836845398\n",
            "Epoch 16, Loss: 1.0712640285491943\n",
            "Epoch 17, Loss: 1.0612385272979736\n",
            "Epoch 18, Loss: 1.0518288612365723\n",
            "Epoch 19, Loss: 1.0429900884628296\n",
            "Epoch 20, Loss: 1.0346808433532715\n",
            "Epoch 21, Loss: 1.0268627405166626\n",
            "Epoch 22, Loss: 1.0195012092590332\n",
            "Epoch 23, Loss: 1.0125638246536255\n",
            "Epoch 24, Loss: 1.0060209035873413\n",
            "Epoch 25, Loss: 0.9998457431793213\n",
            "Epoch 26, Loss: 0.9940021634101868\n",
            "Epoch 27, Loss: 0.9885163307189941\n",
            "Epoch 28, Loss: 0.9836269617080688\n",
            "Epoch 29, Loss: 0.9789995551109314\n",
            "Epoch 30, Loss: 0.9746167659759521\n",
            "Epoch 31, Loss: 0.9704657196998596\n",
            "Epoch 32, Loss: 0.9665452837944031\n",
            "Epoch 33, Loss: 0.9628297686576843\n",
            "Epoch 34, Loss: 0.9592674970626831\n",
            "Epoch 35, Loss: 0.9558789134025574\n",
            "Epoch 36, Loss: 0.9526531100273132\n",
            "Epoch 37, Loss: 0.9495751261711121\n",
            "Epoch 38, Loss: 0.9466260671615601\n",
            "Epoch 39, Loss: 0.9438117146492004\n",
            "Epoch 40, Loss: 0.9411239624023438\n",
            "Epoch 41, Loss: 0.9385572075843811\n",
            "Epoch 42, Loss: 0.9361048936843872\n",
            "Epoch 43, Loss: 0.9337563514709473\n",
            "Epoch 44, Loss: 0.9315059185028076\n",
            "Epoch 45, Loss: 0.9293463826179504\n",
            "Epoch 46, Loss: 0.9272732734680176\n",
            "Epoch 47, Loss: 0.9252803325653076\n",
            "Epoch 48, Loss: 0.9233661890029907\n",
            "Epoch 49, Loss: 0.9215377569198608\n",
            "Epoch 50, Loss: 0.9197754859924316\n",
            "Epoch 51, Loss: 0.9180797338485718\n",
            "Epoch 52, Loss: 0.9164409637451172\n",
            "Epoch 53, Loss: 0.9148523211479187\n",
            "Epoch 54, Loss: 0.9133147597312927\n",
            "Epoch 55, Loss: 0.9118240475654602\n",
            "Epoch 56, Loss: 0.9103766083717346\n",
            "Epoch 57, Loss: 0.9089748859405518\n",
            "Epoch 58, Loss: 0.907615065574646\n",
            "Epoch 59, Loss: 0.9062771797180176\n",
            "Epoch 60, Loss: 0.9049785137176514\n",
            "Epoch 61, Loss: 0.903715193271637\n",
            "Epoch 62, Loss: 0.9024837017059326\n",
            "Epoch 63, Loss: 0.9013401865959167\n",
            "Epoch 64, Loss: 0.9002782702445984\n",
            "Epoch 65, Loss: 0.8992419242858887\n",
            "Epoch 66, Loss: 0.8982295393943787\n",
            "Epoch 67, Loss: 0.8972398638725281\n",
            "Epoch 68, Loss: 0.8962993621826172\n",
            "Epoch 69, Loss: 0.8953956365585327\n",
            "Epoch 70, Loss: 0.8945111632347107\n",
            "Epoch 71, Loss: 0.8936442732810974\n",
            "Epoch 72, Loss: 0.8927942514419556\n",
            "Epoch 73, Loss: 0.8919604420661926\n",
            "Epoch 74, Loss: 0.8911415338516235\n",
            "Epoch 75, Loss: 0.8903369307518005\n",
            "Epoch 76, Loss: 0.8895457983016968\n",
            "Epoch 77, Loss: 0.8887674808502197\n",
            "Epoch 78, Loss: 0.8880012035369873\n",
            "Epoch 79, Loss: 0.8872411847114563\n",
            "Epoch 80, Loss: 0.8864754438400269\n",
            "Epoch 81, Loss: 0.8857197761535645\n",
            "Epoch 82, Loss: 0.8849735856056213\n",
            "Epoch 83, Loss: 0.8842365741729736\n",
            "Epoch 84, Loss: 0.8835081458091736\n",
            "Epoch 85, Loss: 0.8827877640724182\n",
            "Epoch 86, Loss: 0.8820751309394836\n",
            "Epoch 87, Loss: 0.8813697099685669\n",
            "Epoch 88, Loss: 0.8806713819503784\n",
            "Epoch 89, Loss: 0.8799794912338257\n",
            "Epoch 90, Loss: 0.8792937994003296\n",
            "Epoch 91, Loss: 0.878614068031311\n",
            "Epoch 92, Loss: 0.8779398202896118\n",
            "Epoch 93, Loss: 0.8772708177566528\n",
            "Epoch 94, Loss: 0.876606822013855\n",
            "Epoch 95, Loss: 0.8759476542472839\n",
            "Epoch 96, Loss: 0.8752930164337158\n",
            "Epoch 97, Loss: 0.8746370077133179\n",
            "Epoch 98, Loss: 0.8739729523658752\n",
            "Epoch 99, Loss: 0.8733128905296326\n",
            "Epoch 100, Loss: 0.8726567625999451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a recurrent neural network"
      ],
      "metadata": {
        "id": "44OnJTVmYLPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0) # get RNN output\n",
        "        out = self.fc(out[:, -1, :]) # get last output\n",
        "        return out\n",
        "\n",
        "rnn = RNN(10, 20, 1)\n",
        "print(rnn)"
      ],
      "metadata": {
        "id": "hDRCM68LYEsJ",
        "outputId": "15f50df1-84ab-4cf9-936c-d927c199287e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(10, 20, batch_first=True)\n",
            "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(100, 5, 10)\n",
        "Y = torch.randn(100, 1)"
      ],
      "metadata": {
        "id": "adqxb5oQYS_f"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    output = rnn(X)\n",
        "    loss = loss_fn(output, Y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "FyFo1JU2YsZr",
        "outputId": "c4b25af6-8a44-4413-e1f3-ea51203b1d23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.2307977676391602\n",
            "Epoch 2, Loss: 1.2183032035827637\n",
            "Epoch 3, Loss: 1.2067809104919434\n",
            "Epoch 4, Loss: 1.196129322052002\n",
            "Epoch 5, Loss: 1.1862598657608032\n",
            "Epoch 6, Loss: 1.1770938634872437\n",
            "Epoch 7, Loss: 1.1685622930526733\n",
            "Epoch 8, Loss: 1.1606037616729736\n",
            "Epoch 9, Loss: 1.1531637907028198\n",
            "Epoch 10, Loss: 1.1461939811706543\n",
            "Epoch 11, Loss: 1.139650821685791\n",
            "Epoch 12, Loss: 1.1334956884384155\n",
            "Epoch 13, Loss: 1.1276938915252686\n",
            "Epoch 14, Loss: 1.122214436531067\n",
            "Epoch 15, Loss: 1.1170291900634766\n",
            "Epoch 16, Loss: 1.1121132373809814\n",
            "Epoch 17, Loss: 1.1074436902999878\n",
            "Epoch 18, Loss: 1.103000283241272\n",
            "Epoch 19, Loss: 1.0987643003463745\n",
            "Epoch 20, Loss: 1.0947190523147583\n",
            "Epoch 21, Loss: 1.090849757194519\n",
            "Epoch 22, Loss: 1.0871423482894897\n",
            "Epoch 23, Loss: 1.0835844278335571\n",
            "Epoch 24, Loss: 1.0801647901535034\n",
            "Epoch 25, Loss: 1.0768733024597168\n",
            "Epoch 26, Loss: 1.0737003087997437\n",
            "Epoch 27, Loss: 1.0706374645233154\n",
            "Epoch 28, Loss: 1.0676769018173218\n",
            "Epoch 29, Loss: 1.0648115873336792\n",
            "Epoch 30, Loss: 1.062035322189331\n",
            "Epoch 31, Loss: 1.0593419075012207\n",
            "Epoch 32, Loss: 1.0567259788513184\n",
            "Epoch 33, Loss: 1.0541826486587524\n",
            "Epoch 34, Loss: 1.0517072677612305\n",
            "Epoch 35, Loss: 1.0492957830429077\n",
            "Epoch 36, Loss: 1.0469441413879395\n",
            "Epoch 37, Loss: 1.0446490049362183\n",
            "Epoch 38, Loss: 1.0424067974090576\n",
            "Epoch 39, Loss: 1.0402151346206665\n",
            "Epoch 40, Loss: 1.038070797920227\n",
            "Epoch 41, Loss: 1.0359711647033691\n",
            "Epoch 42, Loss: 1.0339139699935913\n",
            "Epoch 43, Loss: 1.031896948814392\n",
            "Epoch 44, Loss: 1.0299181938171387\n",
            "Epoch 45, Loss: 1.0279756784439087\n",
            "Epoch 46, Loss: 1.0260676145553589\n",
            "Epoch 47, Loss: 1.024192452430725\n",
            "Epoch 48, Loss: 1.0223482847213745\n",
            "Epoch 49, Loss: 1.0205341577529907\n",
            "Epoch 50, Loss: 1.0187486410140991\n",
            "Epoch 51, Loss: 1.016990065574646\n",
            "Epoch 52, Loss: 1.0152573585510254\n",
            "Epoch 53, Loss: 1.0135496854782104\n",
            "Epoch 54, Loss: 1.0118658542633057\n",
            "Epoch 55, Loss: 1.010204553604126\n",
            "Epoch 56, Loss: 1.0085651874542236\n",
            "Epoch 57, Loss: 1.0069468021392822\n",
            "Epoch 58, Loss: 1.005348563194275\n",
            "Epoch 59, Loss: 1.0037693977355957\n",
            "Epoch 60, Loss: 1.0022088289260864\n",
            "Epoch 61, Loss: 1.0006660223007202\n",
            "Epoch 62, Loss: 0.9991401433944702\n",
            "Epoch 63, Loss: 0.9976308345794678\n",
            "Epoch 64, Loss: 0.9961373209953308\n",
            "Epoch 65, Loss: 0.9946588277816772\n",
            "Epoch 66, Loss: 0.9931950569152832\n",
            "Epoch 67, Loss: 0.9917453527450562\n",
            "Epoch 68, Loss: 0.9903091192245483\n",
            "Epoch 69, Loss: 0.9888858795166016\n",
            "Epoch 70, Loss: 0.9874751567840576\n",
            "Epoch 71, Loss: 0.9860764145851135\n",
            "Epoch 72, Loss: 0.984689474105835\n",
            "Epoch 73, Loss: 0.9833135008811951\n",
            "Epoch 74, Loss: 0.9819483757019043\n",
            "Epoch 75, Loss: 0.9805935025215149\n",
            "Epoch 76, Loss: 0.9792486429214478\n",
            "Epoch 77, Loss: 0.9779134392738342\n",
            "Epoch 78, Loss: 0.9765875339508057\n",
            "Epoch 79, Loss: 0.9752703309059143\n",
            "Epoch 80, Loss: 0.9739617109298706\n",
            "Epoch 81, Loss: 0.9726613759994507\n",
            "Epoch 82, Loss: 0.9713687896728516\n",
            "Epoch 83, Loss: 0.9700839519500732\n",
            "Epoch 84, Loss: 0.9688063263893127\n",
            "Epoch 85, Loss: 0.9675357937812805\n",
            "Epoch 86, Loss: 0.9662719964981079\n",
            "Epoch 87, Loss: 0.9650146961212158\n",
            "Epoch 88, Loss: 0.9637635946273804\n",
            "Epoch 89, Loss: 0.9625184535980225\n",
            "Epoch 90, Loss: 0.9612790942192078\n",
            "Epoch 91, Loss: 0.9600451588630676\n",
            "Epoch 92, Loss: 0.9588164687156677\n",
            "Epoch 93, Loss: 0.9575928449630737\n",
            "Epoch 94, Loss: 0.9563740491867065\n",
            "Epoch 95, Loss: 0.9551597833633423\n",
            "Epoch 96, Loss: 0.9539499878883362\n",
            "Epoch 97, Loss: 0.9527444243431091\n",
            "Epoch 98, Loss: 0.9515427947044373\n",
            "Epoch 99, Loss: 0.9503451585769653\n",
            "Epoch 100, Loss: 0.9491509795188904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training an LSTM neural network"
      ],
      "metadata": {
        "id": "_fvk53MzY00u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "    c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "    out, _ = self.lstm(x, (h0, c0)) # get LSTM output\n",
        "    out = self.fc(out[:, -1, :])\n",
        "\n",
        "    return out\n",
        "\n",
        "lstm = LSTM(10, 20, 1) # 10 features, 20 hidden units, 1 output\n",
        "print(lstm)"
      ],
      "metadata": {
        "id": "i_vRz87ZYwkI",
        "outputId": "252f4580-2b10-44e1-d0a6-9b8a59f47801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(10, 20, batch_first=True)\n",
            "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(100, 5, 10)\n",
        "Y = torch.randn(100, 1)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(lstm.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "GjToJzMDZbP9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    output = lstm(X)\n",
        "    loss = loss_fn(output, Y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "Loc05ERBZlqP",
        "outputId": "3fca2056-a8a9-484a-c8f3-8db7cf3ef36f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9905087947845459\n",
            "Epoch 2, Loss: 0.9894904494285583\n",
            "Epoch 3, Loss: 0.9885101318359375\n",
            "Epoch 4, Loss: 0.9875661730766296\n",
            "Epoch 5, Loss: 0.9866567850112915\n",
            "Epoch 6, Loss: 0.9857807159423828\n",
            "Epoch 7, Loss: 0.9849362969398499\n",
            "Epoch 8, Loss: 0.984122097492218\n",
            "Epoch 9, Loss: 0.9833366870880127\n",
            "Epoch 10, Loss: 0.9825788736343384\n",
            "Epoch 11, Loss: 0.9818474054336548\n",
            "Epoch 12, Loss: 0.9811410307884216\n",
            "Epoch 13, Loss: 0.980458676815033\n",
            "Epoch 14, Loss: 0.979799211025238\n",
            "Epoch 15, Loss: 0.9791616201400757\n",
            "Epoch 16, Loss: 0.978545069694519\n",
            "Epoch 17, Loss: 0.9779481291770935\n",
            "Epoch 18, Loss: 0.9773706197738647\n",
            "Epoch 19, Loss: 0.9768112301826477\n",
            "Epoch 20, Loss: 0.9762691259384155\n",
            "Epoch 21, Loss: 0.9757438898086548\n",
            "Epoch 22, Loss: 0.9752342700958252\n",
            "Epoch 23, Loss: 0.9747399091720581\n",
            "Epoch 24, Loss: 0.9742599725723267\n",
            "Epoch 25, Loss: 0.9737939238548279\n",
            "Epoch 26, Loss: 0.973341166973114\n",
            "Epoch 27, Loss: 0.9729009866714478\n",
            "Epoch 28, Loss: 0.9724728465080261\n",
            "Epoch 29, Loss: 0.9720562696456909\n",
            "Epoch 30, Loss: 0.9716507792472839\n",
            "Epoch 31, Loss: 0.9712556600570679\n",
            "Epoch 32, Loss: 0.9708707332611084\n",
            "Epoch 33, Loss: 0.9704954028129578\n",
            "Epoch 34, Loss: 0.9701291918754578\n",
            "Epoch 35, Loss: 0.9697718620300293\n",
            "Epoch 36, Loss: 0.9694228172302246\n",
            "Epoch 37, Loss: 0.9690819382667542\n",
            "Epoch 38, Loss: 0.9687485694885254\n",
            "Epoch 39, Loss: 0.9684227108955383\n",
            "Epoch 40, Loss: 0.9681038856506348\n",
            "Epoch 41, Loss: 0.9677914977073669\n",
            "Epoch 42, Loss: 0.9674857258796692\n",
            "Epoch 43, Loss: 0.9671859741210938\n",
            "Epoch 44, Loss: 0.9668923020362854\n",
            "Epoch 45, Loss: 0.9666040539741516\n",
            "Epoch 46, Loss: 0.9663212299346924\n",
            "Epoch 47, Loss: 0.9660434722900391\n",
            "Epoch 48, Loss: 0.9657706618309021\n",
            "Epoch 49, Loss: 0.9655026197433472\n",
            "Epoch 50, Loss: 0.9652388691902161\n",
            "Epoch 51, Loss: 0.9649795293807983\n",
            "Epoch 52, Loss: 0.9647244215011597\n",
            "Epoch 53, Loss: 0.9644731283187866\n",
            "Epoch 54, Loss: 0.964225709438324\n",
            "Epoch 55, Loss: 0.9639816880226135\n",
            "Epoch 56, Loss: 0.9637413024902344\n",
            "Epoch 57, Loss: 0.9635041952133179\n",
            "Epoch 58, Loss: 0.9632702469825745\n",
            "Epoch 59, Loss: 0.963039219379425\n",
            "Epoch 60, Loss: 0.9628112316131592\n",
            "Epoch 61, Loss: 0.9625859260559082\n",
            "Epoch 62, Loss: 0.9623633027076721\n",
            "Epoch 63, Loss: 0.9621431827545166\n",
            "Epoch 64, Loss: 0.9619256854057312\n",
            "Epoch 65, Loss: 0.9617104530334473\n",
            "Epoch 66, Loss: 0.9614974856376648\n",
            "Epoch 67, Loss: 0.9612865447998047\n",
            "Epoch 68, Loss: 0.9610777497291565\n",
            "Epoch 69, Loss: 0.9608709216117859\n",
            "Epoch 70, Loss: 0.9606660604476929\n",
            "Epoch 71, Loss: 0.9604628682136536\n",
            "Epoch 72, Loss: 0.9602614641189575\n",
            "Epoch 73, Loss: 0.96006178855896\n",
            "Epoch 74, Loss: 0.9598637223243713\n",
            "Epoch 75, Loss: 0.9596670269966125\n",
            "Epoch 76, Loss: 0.9594720602035522\n",
            "Epoch 77, Loss: 0.959278404712677\n",
            "Epoch 78, Loss: 0.9590860605239868\n",
            "Epoch 79, Loss: 0.9588950872421265\n",
            "Epoch 80, Loss: 0.9587054252624512\n",
            "Epoch 81, Loss: 0.9585170149803162\n",
            "Epoch 82, Loss: 0.9583295583724976\n",
            "Epoch 83, Loss: 0.9581432938575745\n",
            "Epoch 84, Loss: 0.9579580426216125\n",
            "Epoch 85, Loss: 0.9577740430831909\n",
            "Epoch 86, Loss: 0.9575908780097961\n",
            "Epoch 87, Loss: 0.9574086666107178\n",
            "Epoch 88, Loss: 0.9572274088859558\n",
            "Epoch 89, Loss: 0.9570470452308655\n",
            "Epoch 90, Loss: 0.9568673968315125\n",
            "Epoch 91, Loss: 0.9566888213157654\n",
            "Epoch 92, Loss: 0.9565109014511108\n",
            "Epoch 93, Loss: 0.9563336968421936\n",
            "Epoch 94, Loss: 0.9561573266983032\n",
            "Epoch 95, Loss: 0.9559814929962158\n",
            "Epoch 96, Loss: 0.9558064341545105\n",
            "Epoch 97, Loss: 0.9556320905685425\n",
            "Epoch 98, Loss: 0.9554582238197327\n",
            "Epoch 99, Loss: 0.9552850127220154\n",
            "Epoch 100, Loss: 0.9551125168800354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a convolutional neural network"
      ],
      "metadata": {
        "id": "P4oMpiNNZuQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,\n",
        "      input_size,\n",
        "      hidden_size,\n",
        "      output_size,\n",
        "      kernel_size,\n",
        "      seq_length):\n",
        "      super(ConvNet, self).__init__()\n",
        "      self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size)\n",
        "      self.fc = nn.Linear(hidden_size*(seq_length-kernel_size+1), output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.transpose(1, 2)\n",
        "      out = torch.relu(self.conv1(x))\n",
        "      out = out.view(out.size(0), -1) # flatten the tensor\n",
        "      out = self.fc(out)\n",
        "\n",
        "      return out\n",
        "\n",
        "convnet = ConvNet(5, 20, 1, 3, 10)\n",
        "print(convnet)"
      ],
      "metadata": {
        "id": "TyOL_Ni6ZruO",
        "outputId": "e1191f62-fafb-4d64-fa7e-9b2bab41f2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (conv1): Conv1d(5, 20, kernel_size=(3,), stride=(1,))\n",
            "  (fc): Linear(in_features=160, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(100, 10, 5)\n",
        "Y = torch.randn(100, 1)"
      ],
      "metadata": {
        "id": "6A0FoTt4Z_xX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(convnet.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(100):\n",
        "    output = convnet(X)\n",
        "    loss = loss_fn(output, Y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "LBS59bqnaEH_",
        "outputId": "de23a150-d88c-4d6d-9b01-e66addee8c90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.1577041149139404\n",
            "Epoch 2, Loss: 1.1472264528274536\n",
            "Epoch 3, Loss: 1.1370139122009277\n",
            "Epoch 4, Loss: 1.127050518989563\n",
            "Epoch 5, Loss: 1.117323637008667\n",
            "Epoch 6, Loss: 1.1078319549560547\n",
            "Epoch 7, Loss: 1.0985636711120605\n",
            "Epoch 8, Loss: 1.0895147323608398\n",
            "Epoch 9, Loss: 1.0806804895401\n",
            "Epoch 10, Loss: 1.072043776512146\n",
            "Epoch 11, Loss: 1.0635981559753418\n",
            "Epoch 12, Loss: 1.0553371906280518\n",
            "Epoch 13, Loss: 1.0472521781921387\n",
            "Epoch 14, Loss: 1.0393387079238892\n",
            "Epoch 15, Loss: 1.0315912961959839\n",
            "Epoch 16, Loss: 1.024001955986023\n",
            "Epoch 17, Loss: 1.016566514968872\n",
            "Epoch 18, Loss: 1.0092804431915283\n",
            "Epoch 19, Loss: 1.0021404027938843\n",
            "Epoch 20, Loss: 0.9951435923576355\n",
            "Epoch 21, Loss: 0.9882773756980896\n",
            "Epoch 22, Loss: 0.9815418720245361\n",
            "Epoch 23, Loss: 0.9749428033828735\n",
            "Epoch 24, Loss: 0.9684750437736511\n",
            "Epoch 25, Loss: 0.9621230363845825\n",
            "Epoch 26, Loss: 0.9558832049369812\n",
            "Epoch 27, Loss: 0.9497590661048889\n",
            "Epoch 28, Loss: 0.943746030330658\n",
            "Epoch 29, Loss: 0.9378422498703003\n",
            "Epoch 30, Loss: 0.9320439696311951\n",
            "Epoch 31, Loss: 0.9263458251953125\n",
            "Epoch 32, Loss: 0.9207452535629272\n",
            "Epoch 33, Loss: 0.9152412414550781\n",
            "Epoch 34, Loss: 0.9098309278488159\n",
            "Epoch 35, Loss: 0.9045110940933228\n",
            "Epoch 36, Loss: 0.8992756605148315\n",
            "Epoch 37, Loss: 0.894115686416626\n",
            "Epoch 38, Loss: 0.889037549495697\n",
            "Epoch 39, Loss: 0.8840380311012268\n",
            "Epoch 40, Loss: 0.8791223168373108\n",
            "Epoch 41, Loss: 0.874285876750946\n",
            "Epoch 42, Loss: 0.8695192933082581\n",
            "Epoch 43, Loss: 0.8648235201835632\n",
            "Epoch 44, Loss: 0.8601978421211243\n",
            "Epoch 45, Loss: 0.8556404113769531\n",
            "Epoch 46, Loss: 0.8511467576026917\n",
            "Epoch 47, Loss: 0.8467177748680115\n",
            "Epoch 48, Loss: 0.84235018491745\n",
            "Epoch 49, Loss: 0.8380440473556519\n",
            "Epoch 50, Loss: 0.8337979316711426\n",
            "Epoch 51, Loss: 0.8296077251434326\n",
            "Epoch 52, Loss: 0.8254730105400085\n",
            "Epoch 53, Loss: 0.8213931322097778\n",
            "Epoch 54, Loss: 0.8173670172691345\n",
            "Epoch 55, Loss: 0.8133947253227234\n",
            "Epoch 56, Loss: 0.809476375579834\n",
            "Epoch 57, Loss: 0.8056090474128723\n",
            "Epoch 58, Loss: 0.8017879724502563\n",
            "Epoch 59, Loss: 0.7980177998542786\n",
            "Epoch 60, Loss: 0.7942979335784912\n",
            "Epoch 61, Loss: 0.7906275987625122\n",
            "Epoch 62, Loss: 0.7870001792907715\n",
            "Epoch 63, Loss: 0.7834178805351257\n",
            "Epoch 64, Loss: 0.7798805236816406\n",
            "Epoch 65, Loss: 0.7763864398002625\n",
            "Epoch 66, Loss: 0.7729446887969971\n",
            "Epoch 67, Loss: 0.769548237323761\n",
            "Epoch 68, Loss: 0.766197681427002\n",
            "Epoch 69, Loss: 0.7628837823867798\n",
            "Epoch 70, Loss: 0.7596110701560974\n",
            "Epoch 71, Loss: 0.7563748359680176\n",
            "Epoch 72, Loss: 0.7531754970550537\n",
            "Epoch 73, Loss: 0.7500138282775879\n",
            "Epoch 74, Loss: 0.7468811273574829\n",
            "Epoch 75, Loss: 0.7437834143638611\n",
            "Epoch 76, Loss: 0.7407252788543701\n",
            "Epoch 77, Loss: 0.7376981973648071\n",
            "Epoch 78, Loss: 0.734693706035614\n",
            "Epoch 79, Loss: 0.7317194938659668\n",
            "Epoch 80, Loss: 0.7287753224372864\n",
            "Epoch 81, Loss: 0.7258617281913757\n",
            "Epoch 82, Loss: 0.7229745984077454\n",
            "Epoch 83, Loss: 0.720116376876831\n",
            "Epoch 84, Loss: 0.7172840237617493\n",
            "Epoch 85, Loss: 0.7144765257835388\n",
            "Epoch 86, Loss: 0.7117055654525757\n",
            "Epoch 87, Loss: 0.7089591026306152\n",
            "Epoch 88, Loss: 0.7062330842018127\n",
            "Epoch 89, Loss: 0.703530490398407\n",
            "Epoch 90, Loss: 0.7008560299873352\n",
            "Epoch 91, Loss: 0.69820237159729\n",
            "Epoch 92, Loss: 0.6955735683441162\n",
            "Epoch 93, Loss: 0.6929683089256287\n",
            "Epoch 94, Loss: 0.6903835535049438\n",
            "Epoch 95, Loss: 0.6878252625465393\n",
            "Epoch 96, Loss: 0.6852927207946777\n",
            "Epoch 97, Loss: 0.6827868819236755\n",
            "Epoch 98, Loss: 0.6803049445152283\n",
            "Epoch 99, Loss: 0.6778431534767151\n",
            "Epoch 100, Loss: 0.6754028797149658\n"
          ]
        }
      ]
    }
  ]
}